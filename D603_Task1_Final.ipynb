{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3892bb-e1ec-4ae3-9dc8-ef2bd9ec8d0d",
   "metadata": {},
   "source": [
    "# Task 1: Classification Data Mining Models - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8454b4a-fd81-4afa-a3b0-b0dbaa2b74ac",
   "metadata": {},
   "source": [
    "### B1. Propose one question relevant to a real-world organizational situation that you will answer using one of the following classification methods\n",
    "\n",
    "<p>One question that is relevant to a real-world organizational situation that I will answer using the Random Forest classification method is: What factors are the most predictive of patient readmission within a month of release?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad0dcd-611f-4d37-b8f4-5f00a34e70e2",
   "metadata": {},
   "source": [
    "### B2. Define one goal of the data analysis\n",
    "\n",
    "<p>The goal of this data analysis is to identify the most significant factors that predict patient readmission within a month of release, using demographic, medical, and hospitalization data. This will help the hospital chain to prioritize targeted interventions and resource allocation to reduce readmission rates and avoid penalties from organizations like the Centers for Medicare and Medicaid Services (CMS).</p>\n",
    "\n",
    "<p>This goal is reasonable as the dataset includes relevant variables such as patient demographics, medical conditions, and hospitalization details, which are related to the problem of patient readmissions.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bb359-2b8c-4066-995d-7e5f10cd58e0",
   "metadata": {},
   "source": [
    "### C1. Explain how the classification method you chose analyzes the selected dataset. Include expected outcomes.\n",
    "\n",
    "<p>The Random Forest classification method analyzes the medical dataset in multiple steps. The first step involves preprocessing the dataset. The dataset is prepared by handling missing values and encoding categorical variables. Scaling is not required for Random Forest. Relevant features are selected from the dataset, such as patient demographics, medical conditions, and hospitalization details. The target variable, ReAdmis (indicating if a patient was readmitted within a month), is separated from the predictor variables. The second step involves model training. The Random Forest algorithm creates multiple decision trees. Each decision tree in the Random Forest is “built from a different subset of data and features” (Sruthi, 2024, par. 46). This is to reduce overfitting and improve generalization. The target variable, ReAdmis, is used to train the model, with the goal of predicting whether a patient will be readmitted. The third step involves majority voting. Each decision tree in the forest makes a prediction. The final prediction is based on majority voting across all trees to determine the most likely class for each patient. The fourth step involves feature importance. The Random Forest algorithm calculates the importance of each feature by measuring how much it reduces impurity across all decision trees in the forest. This helps identify which factors are most predictive of patient readmission. </p>\n",
    "\n",
    "<p>Using the Random Forest classification method on the medical dataset yields multiple outcomes. The first outcome involves assessing the model’s ability to predict readmissions using metrics like accuracy, precision, and recall. The second outcome involves identifying the most important factors influencing patient readmission. This will help hospitals prioritize intervention strategies. The third outcome involves uncovering patterns such as patients with certain conditions or demographic characteristics being at higher risk of readmission. This can help hospitals design programs, such as improved follow-up care for high-risk patients. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f52d49-cf05-46c5-b7eb-c2f121a84e63",
   "metadata": {},
   "source": [
    "### C2. List the packages or libraries you have chosen for Python or R, and justify how each item on the list supports the analysis\n",
    "\n",
    "<p>There are three Python libraries that I used for the Random Forest classification analysis. First is the pandas library and it was used to load, manipulate, and preprocess the medical dataset. The to_csv() function from the pandas library was also used to export the training, validation, and test datasets as CSV files. Second is the scikit-learn library and it provides the RandomForestClassifier() function, which I used to build and train the Random Forest model. Scikit-learn provides functions like accuracy_score(), precision_score(), and recall_score(), which I used to evaluate the model’s performance. Scikit-learn also includes tools for hyperparameter tuning, such as GridSearchCV() and StratifiedKFold(), to optimize the Random Forest model. Third is the NumPy library and it was used implicitly by scikit-learn for computations during the Random Forest model training and evaluation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6e019-7b77-41b9-8125-2f2e286829a4",
   "metadata": {},
   "source": [
    "### D1. Describe one data preprocessing goal relevant to the classification method from part B1\n",
    "\n",
    "<p>One data preprocessing goal for using the Random Forest classification method is to handle categorical variables by encoding them into numerical formats. The Random Forest algorithm cannot process categorical data directly, as the algorithm needs numerical inputs to compute splits and make predictions. The medical dataset contains several categorical variables like ReAdmis (Yes/No), Gender (Male/Female), Area (Urban/Suburban/Rural), Services (Blood Work or CT Scan), etc. These variables must be converted into numerical format for use in the Random Forest algorithm, typically through label encoding. Label encoding assigns each category a unique integer value.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0ff9b-ed68-4572-99ab-eb2dab7fc065",
   "metadata": {},
   "source": [
    "### D2. Identify the initial dataset variables that you will use to perform the analysis for the classification question from part B1, and classify each variable as continuous or categorical\n",
    "\n",
    "<p>The variables that I will use to perform the analysis for the Random Forest classification question from part B1 are: ReAdmis(categorical), Age(continuous), Gender(categorical), Income(continuous), Marital(categorical), Area(categorical), HighBlood(categorical), Stroke(categorical), Diabetes(categorical), Overweight(categorical), Arthritis(categorical), Complication_risk(categorical), Initial_admin(categorical), Initial_days(continuous), Doc_visits(continuous), VitD_levels(continuous), Full_meals_eaten(continuous), Soft_drink(categorical), vitD_supp(continuous), TotalCharge(continuous), and Additional_charges(continuous). </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b0d2d-51f5-4447-a749-425c3f737409",
   "metadata": {},
   "source": [
    "### D3. Explain each of the steps used to prepare the data for the analysis. Identify the code segment for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be037e9a-6265-47be-a2c6-432d93ad5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#loading the medical dataset\n",
    "df = pd.read_csv('C:/Users/jcaye/Downloads/medical_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d90af2-b222-4bdc-bc50-995301b6d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaseOrder</th>\n",
       "      <th>Customer_id</th>\n",
       "      <th>Interaction</th>\n",
       "      <th>UID</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lng</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalCharge</th>\n",
       "      <th>Additional_charges</th>\n",
       "      <th>Item1</th>\n",
       "      <th>Item2</th>\n",
       "      <th>Item3</th>\n",
       "      <th>Item4</th>\n",
       "      <th>Item5</th>\n",
       "      <th>Item6</th>\n",
       "      <th>Item7</th>\n",
       "      <th>Item8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C412403</td>\n",
       "      <td>8cd49b13-f45a-4b47-a2bd-173ffa932c2f</td>\n",
       "      <td>3a83ddb66e2ae73798bdf1d705dc0932</td>\n",
       "      <td>Eva</td>\n",
       "      <td>AL</td>\n",
       "      <td>Morgan</td>\n",
       "      <td>35621</td>\n",
       "      <td>34.34960</td>\n",
       "      <td>-86.72508</td>\n",
       "      <td>...</td>\n",
       "      <td>3726.702860</td>\n",
       "      <td>17939.403420</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Z919181</td>\n",
       "      <td>d2450b70-0337-4406-bdbb-bc1037f1734c</td>\n",
       "      <td>176354c5eef714957d486009feabf195</td>\n",
       "      <td>Marianna</td>\n",
       "      <td>FL</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>32446</td>\n",
       "      <td>30.84513</td>\n",
       "      <td>-85.22907</td>\n",
       "      <td>...</td>\n",
       "      <td>4193.190458</td>\n",
       "      <td>17612.998120</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>F995323</td>\n",
       "      <td>a2057123-abf5-4a2c-abad-8ffe33512562</td>\n",
       "      <td>e19a0fa00aeda885b8a436757e889bc9</td>\n",
       "      <td>Sioux Falls</td>\n",
       "      <td>SD</td>\n",
       "      <td>Minnehaha</td>\n",
       "      <td>57110</td>\n",
       "      <td>43.54321</td>\n",
       "      <td>-96.63772</td>\n",
       "      <td>...</td>\n",
       "      <td>2434.234222</td>\n",
       "      <td>17505.192460</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A879973</td>\n",
       "      <td>1dec528d-eb34-4079-adce-0d7a40e82205</td>\n",
       "      <td>cd17d7b6d152cb6f23957346d11c3f07</td>\n",
       "      <td>New Richland</td>\n",
       "      <td>MN</td>\n",
       "      <td>Waseca</td>\n",
       "      <td>56072</td>\n",
       "      <td>43.89744</td>\n",
       "      <td>-93.51479</td>\n",
       "      <td>...</td>\n",
       "      <td>2127.830423</td>\n",
       "      <td>12993.437350</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C544523</td>\n",
       "      <td>5885f56b-d6da-43a3-8760-83583af94266</td>\n",
       "      <td>d2f0425877b10ed6bb381f3e2579424a</td>\n",
       "      <td>West Point</td>\n",
       "      <td>VA</td>\n",
       "      <td>King William</td>\n",
       "      <td>23181</td>\n",
       "      <td>37.59894</td>\n",
       "      <td>-76.88958</td>\n",
       "      <td>...</td>\n",
       "      <td>2113.073274</td>\n",
       "      <td>3716.525786</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CaseOrder Customer_id                           Interaction  \\\n",
       "0          1     C412403  8cd49b13-f45a-4b47-a2bd-173ffa932c2f   \n",
       "1          2     Z919181  d2450b70-0337-4406-bdbb-bc1037f1734c   \n",
       "2          3     F995323  a2057123-abf5-4a2c-abad-8ffe33512562   \n",
       "3          4     A879973  1dec528d-eb34-4079-adce-0d7a40e82205   \n",
       "4          5     C544523  5885f56b-d6da-43a3-8760-83583af94266   \n",
       "\n",
       "                                UID          City State        County    Zip  \\\n",
       "0  3a83ddb66e2ae73798bdf1d705dc0932           Eva    AL        Morgan  35621   \n",
       "1  176354c5eef714957d486009feabf195      Marianna    FL       Jackson  32446   \n",
       "2  e19a0fa00aeda885b8a436757e889bc9   Sioux Falls    SD     Minnehaha  57110   \n",
       "3  cd17d7b6d152cb6f23957346d11c3f07  New Richland    MN        Waseca  56072   \n",
       "4  d2f0425877b10ed6bb381f3e2579424a    West Point    VA  King William  23181   \n",
       "\n",
       "        Lat       Lng  ...  TotalCharge Additional_charges Item1 Item2  Item3  \\\n",
       "0  34.34960 -86.72508  ...  3726.702860       17939.403420     3     3      2   \n",
       "1  30.84513 -85.22907  ...  4193.190458       17612.998120     3     4      3   \n",
       "2  43.54321 -96.63772  ...  2434.234222       17505.192460     2     4      4   \n",
       "3  43.89744 -93.51479  ...  2127.830423       12993.437350     3     5      5   \n",
       "4  37.59894 -76.88958  ...  2113.073274        3716.525786     2     1      3   \n",
       "\n",
       "   Item4  Item5 Item6 Item7 Item8  \n",
       "0      2      4     3     3     4  \n",
       "1      4      4     4     3     3  \n",
       "2      4      3     4     3     3  \n",
       "3      3      4     5     5     5  \n",
       "4      3      5     3     4     3  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26526d4c-6810-48b6-b845-8f5cdab815cb",
   "metadata": {},
   "source": [
    "<p>I used the read_csv() function from the pandas library to load the medical dataset into a DataFrame for analysis. The output from the head() function verifies that the medical dataset was loaded successfully.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9f21e6-f535-4e29-81a3-0dec9f9f22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting relevant columns\n",
    "columns_to_use = [\n",
    "    'ReAdmis', 'Age', 'Gender', 'Income', 'Marital', 'Area',\n",
    "    'HighBlood', 'Stroke', 'Diabetes', 'Overweight', 'Arthritis',\n",
    "    'Complication_risk', 'Initial_admin', 'Initial_days', \n",
    "    'Doc_visits', 'VitD_levels', 'Full_meals_eaten', \n",
    "    'Soft_drink', 'vitD_supp', 'TotalCharge', 'Additional_charges'\n",
    "]\n",
    "df = df[columns_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95e8ec5-94ff-40f8-b7c5-ebe10e48fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   ReAdmis             10000 non-null  object \n",
      " 1   Age                 10000 non-null  int64  \n",
      " 2   Gender              10000 non-null  object \n",
      " 3   Income              10000 non-null  float64\n",
      " 4   Marital             10000 non-null  object \n",
      " 5   Area                10000 non-null  object \n",
      " 6   HighBlood           10000 non-null  object \n",
      " 7   Stroke              10000 non-null  object \n",
      " 8   Diabetes            10000 non-null  object \n",
      " 9   Overweight          10000 non-null  object \n",
      " 10  Arthritis           10000 non-null  object \n",
      " 11  Complication_risk   10000 non-null  object \n",
      " 12  Initial_admin       10000 non-null  object \n",
      " 13  Initial_days        10000 non-null  float64\n",
      " 14  Doc_visits          10000 non-null  int64  \n",
      " 15  VitD_levels         10000 non-null  float64\n",
      " 16  Full_meals_eaten    10000 non-null  int64  \n",
      " 17  Soft_drink          10000 non-null  object \n",
      " 18  vitD_supp           10000 non-null  int64  \n",
      " 19  TotalCharge         10000 non-null  float64\n",
      " 20  Additional_charges  10000 non-null  float64\n",
      "dtypes: float64(5), int64(4), object(12)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28140935-7398-4678-8c8b-d579a813d7a0",
   "metadata": {},
   "source": [
    "<p>These are the variables that I selected for analysis: ReAdmis, Age, Gender, Income, Marital, Area, HighBlood, Stroke, Diabetes, Overweight, Arthritis, Complication_risk, Initial_admin, Initial_days, Doc_visits, VitD_levels, Full_meals_eaten, Soft_drink, vitD_supp, TotalCharge, and Additional_charges. The dataset is filtered to keep only the specified variables, and all other variables are removed. The output from the info() confirms the selected variables exist and shows their data types and non-null value counts. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5603cc44-bed3-4f4d-85ba-7847819f8cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAdmis               0\n",
      "Age                   0\n",
      "Gender                0\n",
      "Income                0\n",
      "Marital               0\n",
      "Area                  0\n",
      "HighBlood             0\n",
      "Stroke                0\n",
      "Diabetes              0\n",
      "Overweight            0\n",
      "Arthritis             0\n",
      "Complication_risk     0\n",
      "Initial_admin         0\n",
      "Initial_days          0\n",
      "Doc_visits            0\n",
      "VitD_levels           0\n",
      "Full_meals_eaten      0\n",
      "Soft_drink            0\n",
      "vitD_supp             0\n",
      "TotalCharge           0\n",
      "Additional_charges    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bfc5d-fd1c-4c2d-89eb-69cb5bed7e8e",
   "metadata": {},
   "source": [
    "<p>I used the isnull() function with the sum() function to count missing values in each column of the medical dataset. Since there are no missing values, we can skip handling them and move straight to encoding categorical variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9130cefb-d323-4324-ae75-e8150c2ade31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#categorical variables to label encode\n",
    "vars_to_encode = [\n",
    "    'ReAdmis', 'Gender', 'Marital', 'Area', 'HighBlood', \n",
    "    'Stroke', 'Diabetes', 'Overweight', 'Arthritis', \n",
    "    'Initial_admin', 'Soft_drink', 'Complication_risk'\n",
    "]\n",
    "\n",
    "#applying label encoding to each variable\n",
    "encoder = LabelEncoder()\n",
    "for var in vars_to_encode:\n",
    "    df[var] = encoder.fit_transform(df[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1024d4c0-b24e-4bf6-a2d8-0fe5199f42f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReAdmis</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income</th>\n",
       "      <th>Marital</th>\n",
       "      <th>Area</th>\n",
       "      <th>HighBlood</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Overweight</th>\n",
       "      <th>...</th>\n",
       "      <th>Complication_risk</th>\n",
       "      <th>Initial_admin</th>\n",
       "      <th>Initial_days</th>\n",
       "      <th>Doc_visits</th>\n",
       "      <th>VitD_levels</th>\n",
       "      <th>Full_meals_eaten</th>\n",
       "      <th>Soft_drink</th>\n",
       "      <th>vitD_supp</th>\n",
       "      <th>TotalCharge</th>\n",
       "      <th>Additional_charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>86575.93</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.585770</td>\n",
       "      <td>6</td>\n",
       "      <td>19.141466</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3726.702860</td>\n",
       "      <td>17939.403420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>46805.99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.129562</td>\n",
       "      <td>4</td>\n",
       "      <td>18.940352</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4193.190458</td>\n",
       "      <td>17612.998120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>14370.14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.772177</td>\n",
       "      <td>4</td>\n",
       "      <td>18.057507</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2434.234222</td>\n",
       "      <td>17505.192460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>39741.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.714879</td>\n",
       "      <td>4</td>\n",
       "      <td>16.576858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2127.830423</td>\n",
       "      <td>12993.437350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1209.56</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.254807</td>\n",
       "      <td>5</td>\n",
       "      <td>17.439069</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2113.073274</td>\n",
       "      <td>3716.525786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReAdmis  Age  Gender    Income  Marital  Area  HighBlood  Stroke  Diabetes  \\\n",
       "0        0   53       1  86575.93        0     1          1       0         1   \n",
       "1        0   51       0  46805.99        1     2          1       0         0   \n",
       "2        0   53       0  14370.14        4     1          1       0         1   \n",
       "3        0   78       1  39741.49        1     1          0       1         0   \n",
       "4        0   22       0   1209.56        4     0          0       0         0   \n",
       "\n",
       "   Overweight  ...  Complication_risk  Initial_admin  Initial_days  \\\n",
       "0           0  ...                  2              1     10.585770   \n",
       "1           1  ...                  0              1     15.129562   \n",
       "2           1  ...                  2              0      4.772177   \n",
       "3           0  ...                  2              0      1.714879   \n",
       "4           0  ...                  1              0      1.254807   \n",
       "\n",
       "   Doc_visits  VitD_levels  Full_meals_eaten  Soft_drink  vitD_supp  \\\n",
       "0           6    19.141466                 0           0          0   \n",
       "1           4    18.940352                 2           0          1   \n",
       "2           4    18.057507                 1           0          0   \n",
       "3           4    16.576858                 1           0          0   \n",
       "4           5    17.439069                 0           1          2   \n",
       "\n",
       "   TotalCharge  Additional_charges  \n",
       "0  3726.702860        17939.403420  \n",
       "1  4193.190458        17612.998120  \n",
       "2  2434.234222        17505.192460  \n",
       "3  2127.830423        12993.437350  \n",
       "4  2113.073274         3716.525786  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234f94b-8c4b-41c5-94b8-87a75c11a9e3",
   "metadata": {},
   "source": [
    "<p>I imported the LabelEncoder() function from the sklearn library to convert categorical variables into numerical values for Random Forest, which only works with numerical inputs. I used a for loop to iterate over each categorical variable and apply label encoding. I used the head() function to display the first 5 rows of the updated medical dataset after label encoding.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74c28f-25c4-460e-b0ec-cdca43732950",
   "metadata": {},
   "source": [
    "### D4. Provide a copy of the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a500b1a-30ab-41b7-9655-ba1bd70939bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the cleaned dataset to a CSV file\n",
    "df.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89237b-f630-4209-9129-9d64a95f60fd",
   "metadata": {},
   "source": [
    "<p>Cleaned_dataset.csv is the medical dataset that has been properly prepared for analysis using the Random Forest classification method. I included this file in my submission. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62822c4f-8758-43ec-861c-0c6bbdf0916e",
   "metadata": {},
   "source": [
    "### E1. Split the data into training, validation and test datasets and provide the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21b883c-b9f6-424a-803c-e4ebc9ad39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#separating the dataset into features and the target variable \n",
    "X = df.drop(columns=['ReAdmis'])\n",
    "y = df['ReAdmis']\n",
    "\n",
    "#splitting the dataset into training + validation (80%) and test set (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9b67a9-e6c4-4c59-a2a1-8e055eda365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training + validation set into separate training (70%) and validation (30%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "413650b6-7ce1-4a08-ab61-e1eaeaae505e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (5600, 20)\n",
      "Validation set: (2400, 20)\n",
      "Test set: (2000, 20)\n"
     ]
    }
   ],
   "source": [
    "#printing dataset sizes to confirm splits\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0b354-e4df-4064-9722-982d16f04b92",
   "metadata": {},
   "source": [
    "<p>Before splitting the data, I separated the predictors and the target variable, ReAdmis. I assigned the predictors to variable X and the target variable to y. Next, I used the train_test_split() function from the sklearn library to split the data into training and test sets. Then I split the training set into training and validation sets. The training set is 56%, the validation set is 24%, and the test set is 20% of the original data. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd8137-7f70-46aa-8ae4-6ed771cf409a",
   "metadata": {},
   "source": [
    "#### Providing the training, validation, and test data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2f7654-2c8f-4062-8cd5-2b963cd41297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining features and target for the training set, and exporting it\n",
    "training_data = X_train.copy()\n",
    "training_data['ReAdmis'] = y_train\n",
    "training_data.to_csv('training_data.csv', index=False)\n",
    "\n",
    "#combining features and target for the validation set, and exporting it\n",
    "validation_data = X_val.copy()\n",
    "validation_data['ReAdmis'] = y_val\n",
    "validation_data.to_csv('validation_data.csv', index=False)\n",
    "\n",
    "#combinining features and target for the test set, and exporting it\n",
    "test_data = X_test.copy()\n",
    "test_data['ReAdmis'] = y_test\n",
    "test_data.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016e8d2-3376-49b6-b75c-67dd2ec74e3b",
   "metadata": {},
   "source": [
    "<p>I combined features and target for the training, validation, and test sets, and exported all three as CSV files. The training set is saved as “training_data.csv.” The validation set is saved as “validation_data.csv.” The test set is saved as “test_data.csv.” </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad965da-5213-4c60-a5d2-cb652f1de659",
   "metadata": {},
   "source": [
    "### E2. Create an initial model using the training dataset and provide a screenshot of the following metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1839ba1-1838-41db-9be8-62e36b73d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "#initializing the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=24)\n",
    "\n",
    "#training the model using the training dataset\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "#making predictions on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "y_pred_prob = rf_model.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989b6feb-2ae8-48e9-a6b7-8ba434ecf7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the initial Random Forest model on the validation dataset\n",
      "\n",
      "Accuracy: 0.98\n",
      "Precision: 0.97\n",
      "Recall: 0.97\n",
      "F1 Score: 0.97\n",
      "AUC-ROC: 1.00\n",
      "Confusion Matrix:\n",
      "[[1435   26]\n",
      " [  26  913]]\n"
     ]
    }
   ],
   "source": [
    "#calculating evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "auc_roc = roc_auc_score(y_val, y_pred_prob)\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "#printing evaluation metrics\n",
    "print(\"Evaluating the initial Random Forest model on the validation dataset\\n\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de9121-25f1-4cc8-90ec-77c2d8404384",
   "metadata": {},
   "source": [
    "<p>In this code, I started by importing the RandomForestClassifier() function from the sklearn library, as this function is used to create and train the Random Forest model. I also imported functions like accuracy_score(), precision_score(), recall_score(), etc., as these functions are used to evaluate the model’s performance. </p>\n",
    "\n",
    "<p>I initialized the Random Forest model by using the RandomForestClassifier() function. Next, I fit the model to the training set. Then, I used the predict() function for the model to predict the class labels for the validation set. I also used the predict_proba() function for the model to predict probabilities for each class. This is used to calculate the AUC-ROC metric.</p>\n",
    "\n",
    "<p>I calculated and displayed the accuracy, precision, recall, F1 score, AUC-ROC, and confusion matrix for the Random Forest model. The model has an accuracy score of 0.98, precision score of 0.97, recall score of 0.97, F1 score of 0.97, and an AUC-ROC of 1.00. The model’s confusion matrix shows 1435 true negatives, 26 false positives, 26 false negatives, and 913 true positives. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94c727-592e-4b75-8828-7f6cd7e235ed",
   "metadata": {},
   "source": [
    "### E3. Perform hyperparameter tuning on the validation dataset using k-fold cross validation to find the optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ba207f-ae0a-4603-91e9-35b9d690722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=24, shuffle=True),\n",
       "             estimator=RandomForestClassifier(random_state=24), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: [5, 10, 20, None],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=24, shuffle=True),\n",
       "             estimator=RandomForestClassifier(random_state=24), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: [5, 10, 20, None],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=24)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=24)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=24, shuffle=True),\n",
       "             estimator=RandomForestClassifier(random_state=24), n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 10, 20, None],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [50, 100, 200]},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "#defining the parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],  #number of trees\n",
    "    \"max_depth\": [5, 10, 20, None],  #max depth of each tree\n",
    "    \"min_samples_split\": [2, 5, 10],  #min samples required to split a node\n",
    "    \"min_samples_leaf\": [1, 2, 4],  #min samples required at each leaf\n",
    "}\n",
    "\n",
    "#initializing the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=24)\n",
    "\n",
    "#defining the cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=24)\n",
    "\n",
    "#initializing GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",  #accuracy is the metric to optimize\n",
    "    cv=cv,  #use 5-fold cross-validation\n",
    "    verbose=2, \n",
    "    n_jobs=-1,  \n",
    ")\n",
    "\n",
    "#performing the grid search\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a6edb-3984-446a-b670-c7ce8469ba63",
   "metadata": {},
   "source": [
    "<p>The hyperparameters that I selected for tuning are ‘n_estimators’, ‘max_depth’, ‘min_samples_split’, and’ min_samples_leaf’. The hyperparameter ‘n_estimators’ represents the number of trees in the forest. The hyperparameter ‘max_depth’ controls the maximum depth of each tree. The hyperparameter ‘min_samples_split’ is the minimum number of samples required to split a node. The hyperparameter ‘min_samples_leaf’ is the minimum number of samples required in a leaf node.</p>\n",
    "\n",
    "<p>Here, I will discuss the justification of the selected hyperparameters. I selected ‘n_estimators’ because it controls the number of trees in the forest, with more trees typically improving performance but increasing training time. I selected ‘max_depth’ because it limits tree depth, helping to prevent overfitting by avoiding overly complex trees. I selected ‘min_samples_split’ because it reduces overfitting by requiring more samples to split a node. I selected ‘min_samples_leaf’ because it reduces overfitting by ensuring leaf nodes have sufficient samples.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c31a9736-70f7-45fd-a261-347708054e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Best Cross-Validation Score: 0.979642857142857\n"
     ]
    }
   ],
   "source": [
    "#finding best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "#displaying the results\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Cross-Validation Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6775a2-1ffb-4657-b928-0a9402c41c36",
   "metadata": {},
   "source": [
    "<p>The optimized Random Forest model’s best hyperparameters are max_depth=10, min_samples_leaf=4, min_samples_split=10, and n_estimators=50. This combination of hyperparameters achieved an average accuracy score of 97.96% during cross-validation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d836bd-126e-49a9-bc68-249cf35ec86c",
   "metadata": {},
   "source": [
    "#### Evaluating the optimized model on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88c1493b-246b-4f2f-9c1e-db2907c310bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the optimized Random Forest model on the validation dataset\n",
      "\n",
      "Validation Accuracy: 0.98\n",
      "Validation Precision: 0.97\n",
      "Validation Recall: 0.97\n",
      "Validation F1 Score: 0.97\n",
      "Validation AUC-ROC: 1.00\n",
      "Validation Confusion Matrix:\n",
      "[[1437   24]\n",
      " [  25  914]]\n"
     ]
    }
   ],
   "source": [
    "#training the model with the best hyperparameters\n",
    "best_rf_model = RandomForestClassifier(**grid_search.best_params_, random_state=24)\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "#making predictions on the validation set\n",
    "y_val_pred = best_rf_model.predict(X_val)\n",
    "y_val_pred_prob = best_rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "#calculating evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred)\n",
    "recall = recall_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "auc_roc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "#displaying metrics\n",
    "print(\"Evaluating the optimized Random Forest model on the validation dataset\\n\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Validation Precision: {precision:.2f}\")\n",
    "print(f\"Validation Recall: {recall:.2f}\")\n",
    "print(f\"Validation F1 Score: {f1:.2f}\")\n",
    "print(f\"Validation AUC-ROC: {auc_roc:.2f}\")\n",
    "print(f\"Validation Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc5d84-f032-4610-a2ca-e259bbc844cb",
   "metadata": {},
   "source": [
    "<p>I evaluated the optimized Random Forest model on the validation dataset. The optimized model has an accuracy score of 0.98, precision score of 0.97, recall score of 0.97, F1 score of 0.97, and an AUC-ROC of 1.00. The model’s confusion matrix shows 1437 true negatives, 24 false positives, 25 false negatives, and 914 true positives.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3824dd-9973-4886-b9f5-398df2df3a20",
   "metadata": {},
   "source": [
    "### E4. Use the optimized model identified in part E3 to make predictions using the test dataset and provide a screenshot of the following metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfb0b792-47a3-4cab-83f8-5474a8d9ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the optimized Random Forest model on the test dataset\n",
      "\n",
      "Test Accuracy: 0.98\n",
      "Test Precision: 0.98\n",
      "Test Recall: 0.98\n",
      "Test F1 Score: 0.98\n",
      "Test AUC-ROC: 1.00\n",
      "Test Confusion Matrix:\n",
      "[[1250   15]\n",
      " [  16  719]]\n"
     ]
    }
   ],
   "source": [
    "#making predictions on the test set\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "y_test_pred_prob = best_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#calculating evaluation metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_auc_roc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "#displaying metrics\n",
    "print(\"Evaluating the optimized Random Forest model on the test dataset\\n\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Test Precision: {test_precision:.2f}\")\n",
    "print(f\"Test Recall: {test_recall:.2f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.2f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc_roc:.2f}\")\n",
    "print(f\"Test Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5cc76d-3639-4002-be9f-3b131a7a8df7",
   "metadata": {},
   "source": [
    "<p>I evaluated the optimized Random Forest model on the test dataset. The optimized model has an accuracy score of 0.98, precision score of 0.98, recall score of 0.98, F1 score of 0.98, and an AUC-ROC of 1.00. The model’s confusion matrix shows 1250 true negatives, 15 false positives, 16 false negatives, and 719 true positives. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2366a3-4fc7-4426-afa3-410e096b3087",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f53d8e36-76b1-4141-ba03-a03bbdd3d052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial_days: 0.5439\n",
      "TotalCharge: 0.4190\n",
      "Additional_charges: 0.0060\n",
      "Income: 0.0058\n",
      "VitD_levels: 0.0057\n",
      "Age: 0.0039\n",
      "Complication_risk: 0.0027\n",
      "Marital: 0.0019\n",
      "Initial_admin: 0.0018\n",
      "Full_meals_eaten: 0.0015\n",
      "Doc_visits: 0.0013\n",
      "Area: 0.0013\n",
      "Arthritis: 0.0009\n",
      "HighBlood: 0.0008\n",
      "vitD_supp: 0.0008\n",
      "Gender: 0.0007\n",
      "Overweight: 0.0007\n",
      "Stroke: 0.0005\n",
      "Soft_drink: 0.0005\n",
      "Diabetes: 0.0005\n"
     ]
    }
   ],
   "source": [
    "#getting features importances\n",
    "importances = best_rf_model.feature_importances_\n",
    "\n",
    "#pairing feature names with their importance values\n",
    "feature_importance = sorted(zip(X_train.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#printing top features\n",
    "for feature, importance in feature_importance:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d07d49-a22e-43e4-ab6f-d18ce4fd94a8",
   "metadata": {},
   "source": [
    "<p>I extracted and sorted the feature importance values from the optimized Random Forest model, ranking features by their contribution to predicting patient readmissions within a month. The most significant predictor of readmission is ‘Initial_days’ with an importance value of 0.5439. This suggests that longer stays may indicate severe health issues, increasing the likelihood of readmission. The second most significant predictor is ‘TotalCharge’ with an importance value of 0.4190. This suggests that higher charges may indicate intensive treatments or complex conditions, which correlate with higher readmission risks. Medical conditions like arthritis, high blood pressure, stroke, and diabetes have very low importance values, indicating they contribute minimally to the model’s predictions. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938170d5-0395-4f0f-8808-23bee02c2237",
   "metadata": {},
   "source": [
    "### F1. Compare and discuss the metrics of accuracy, precision, recall, F1 score, and AUC-ROC from the use of the optimized model on the test dataset and the initial model on the training dataset to evaluate the performance of the optimized model\n",
    "\n",
    "<p>Here we will compare and discuss the metrics between the initial Random Forest model that was evaluated on the validation dataset and the optimized Random Forest model that was evaluated on the test dataset. Both initial and optimized models achieved an accuracy of 98%, which means both models are very effective at correctly classifying patient readmissions. The optimized model’s performance on the test dataset confirms that the model generalizes well to new data. Precision improved slightly in the optimized model, increasing from 97% to 98%. This indicates that the optimized model is slightly better at reducing false positives, making it more accurate in identifying patients likely to be readmitted. Recall also improved slightly in the optimized model, increasing from 97% to 98%. This indicates that the optimized model is slightly better at identifying true positives, making it more effective at detecting patients who will actually be readmitted. The F1 score improved from 0.97 in the initial model to 0.98 in the optimized model. This improvement shows a better balance between precision and recall in the optimized model. Both models achieved an AUC-ROC of 1.00, indicating that they are excellent at distinguishing between patients who will and will not be readmitted. This consistency shows the optimized model maintained the excellent class separation of the initial model. </p>\n",
    "\n",
    "<p>Overall, the optimized Random Forest model showed slight improvements in precision, recall, and F1 score compared to the initial model. These improvements demonstrate more accurate identification of true positives with fewer false positives. These improvements also suggest that hyperparameter tuning and cross-validation successfully refined the Random Forest model, making it more reliable for predicting patient readmissions on unseen data.   </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9fa6d5-c205-400a-8ee1-ced56bbbdeea",
   "metadata": {},
   "source": [
    "### F2. Discuss the results and implications of your classification analysis\n",
    "\n",
    "<p>The Random Forest classification analysis on the medical dataset produced strong results, as highlighted by the key metrics. The optimized model achieved an accuracy score of 98%, indicating excellent performance in correctly classifying patient readmissions. The model achieved a precision score of 98%, showing excellent performance in reducing false positives and reliably identifying patients likely to be readmitted. The model achieved a recall score of 98%, indicating that the model is very effective at identifying patients who will actually be readmitted. The model achieved an F1 score of 98%, showing a near-perfect balance between precision and recall, making it reliable for predicting readmissions. The model achieved an AUC-ROC of 1.00, demonstrating excellent ability in distinguishing between patients who will and will not be readmitted. </p>\n",
    "\n",
    "<p>Regarding the results of performing feature importance analysis in the optimized Random Forest model, there are two features that significantly influence patient readmission. The most important feature is ‘Initial_days’ with an importance value of 0.5439. This suggests that longer stays could reflect serious health problems, raising the chances of readmission. The second most important feature is ‘TotalCharge’ with an importance value of 0.4190. This suggests that higher charges might reflect intensive treatments or complex conditions, which are associated with increased readmission risks. Medical condition features like arthritis, high blood pressure, stroke, and diabetes have very low importance values, showing they have little impact on the model’s predictions.</p>\n",
    "\n",
    "<p>The analysis has several important implications. The first implication is that the optimized model can accurately identify patients at risk of readmission, since it has high precision and recall. This allows hospitals to focus resources on high-risk patients, potentially reducing readmission rates and improving patient outcomes. The second implication is that the optimized model reduces false positives, minimizing unnecessary interventions for low-risk patients. The third implication is that hospitals can use this model to predict readmissions proactively, helping reduce penalties from organizations like CMS. The fourth implication is hospitals can prioritize interventions for patients with long hospital stays and high treatment costs, as these factors often indicate higher readmission risks. The fifth implication is hospitals can simplify their predictive models by focusing on key features like ‘Initial_days’ and ‘TotalCharge’, reducing complexity while maintaining accuracy.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211398b3-e375-4682-aea2-741adb4817f3",
   "metadata": {},
   "source": [
    "### F3.  Discuss one limitation of your data analysis\n",
    "\n",
    "<p>One limitation of the Random Forest data analysis is it lacks straightforward interpretability compared to simpler models like logistic regression. While feature importance provides some interpretability, it does not detail the relationships between predictors and the target variable, such as how ‘Initial_days’ specifically influences readmission risk. This makes it harder for healthcare professionals to understand or trust the model’s predictions, which is critical in healthcare decision-making.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f8fee2-5ee7-43b3-9113-0a01992e4fc1",
   "metadata": {},
   "source": [
    "### F4. Recommend a course of action for the real-world organizational situation from part B1 based on your results and implications discussed in part F2\n",
    "\n",
    "<p>One recommended course of action to address the question “What factors are the most predictive of patient readmission within a month of release?” is to focus on high-risk predictors such as ‘Initial_days’ and ‘TotalCharge.’ Hospitals should improve discharge planning for patients with longer hospital stays, as they face a higher risk of readmission. Hospitals should provide additional follow-up care, such as home visits, telehealth consultations or regular check-ins for these patients. Patients with higher hospital charges often have more complex medical conditions. Hospitals should create personalized post-discharge care plans for these patients to address potential complications early. Overall, hospitals should prioritize resources for patients with the highest risk scores and reduce unnecessary interventions for low-risk patients to save time and costs. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268df9ed-ff83-46a8-b46d-16750a201829",
   "metadata": {},
   "source": [
    "### G. Panopto Video\n",
    "\n",
    "<p>Panopto Video Link: https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=00a8caac-64cf-4544-9387-b26e001a4503</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e22033-370f-4dd9-ba3e-0123fca56c17",
   "metadata": {},
   "source": [
    "### H. Record the web sources used to acquire data or segments of third-party code to support the analysis. Ensure the web sources are reliable\n",
    "\n",
    "<p>\n",
    "Firdose, T. (2023, August 24). <i>Fine-tuning your random forest classifier: A guide to hyperparameter tuning</i>. Medium. https://tahera-firdose.medium.com/fine-tuning-your-random-forest-classifier-a-guide-to-hyperparameter-tuning-d5ceab0c4852\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cd76d-fd3a-4644-8b9a-a75009bc0ee0",
   "metadata": {},
   "source": [
    "### I. Acknowledge sources, using in-text citations and references, for content that is quoted, paraphrased, or summarized\n",
    "\n",
    "<p>Sruthi. (2024, December 11). <i>Understanding random forest algorithm with examples</i>. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c99c2a-c1a4-4abd-9826-815cf8a2d48f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
